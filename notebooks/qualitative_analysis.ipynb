{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9770c0bf",
   "metadata": {},
   "source": [
    "# AI-Enhanced Qualitative Data Analysis\n",
    "\n",
    "This notebook processes Excel workbooks containing qualitative data and uses AI models (BERTopic or Top2Vec) to automatically extract themes and topics for qualitative data analysis (QDA).\n",
    "\n",
    "## Features\n",
    "- Load multiple Excel sheets/tabs\n",
    "- Extract qualitative text from specified columns\n",
    "- Apply BERTopic for hierarchical theme extraction\n",
    "- Interactive visualizations\n",
    "- Export results for QDA tools (Taguette, QualCoder, etc.)\n",
    "\n",
    "## Setup\n",
    "1. Place your Excel workbook in the `data/raw/` directory\n",
    "2. Update the `excel_file` path below\n",
    "3. Customize column names if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c753fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "# Uncomment the line below if running in an environment without these packages\n",
    "# !uv install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa16acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280f4cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel file: ../data/raw/ALFD Call Center 10-1-2021 to 12-10-2025 - RAW.xlsx\n",
      "Loading workbook (this may take a moment for large files)...\n",
      "âœ“ Successfully loaded workbook in 1.9 seconds\n",
      "Loaded 3 sheets: ['COLA FONL Unique', 'General Inqury Unique', 'Contact Reason Details']\n",
      "  - COLA FONL Unique: 2,517 rows, 11 columns\n",
      "  - General Inqury Unique: 1,583 rows, 11 columns\n",
      "  - Contact Reason Details: 31,879 rows, 11 columns\n",
      "\n",
      "COMPLETE - Total rows across all sheets: 35,979\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the Excel workbook\n",
    "# Replace 'your_workbook.xlsx' with the path to your Excel file\n",
    "# Find the first .xlsx file in the data/raw directory\n",
    "import time\n",
    "excel_files = glob.glob('../data/raw/*.xlsx')\n",
    "\n",
    "if excel_files:\n",
    "    excel_file = excel_files[0]  # Use the first Excel file found\n",
    "    print(f\"Found Excel file: {excel_file}\")\n",
    "    \n",
    "    # Load all sheets into a dictionary of DataFrames\n",
    "    try:\n",
    "        print(\"Loading workbook (this may take a moment for large files)...\")\n",
    "        start_time = time.time()\n",
    "        sheets_dict = pd.read_excel(excel_file, sheet_name=None)\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ“ Successfully loaded workbook in {load_time:.1f} seconds\")\n",
    "        print(f\"Loaded {len(sheets_dict)} sheets: {list(sheets_dict.keys())}\")\n",
    "        \n",
    "        # Display basic info about each sheet\n",
    "        total_rows = 0\n",
    "        for sheet_name, df in sheets_dict.items():\n",
    "            rows = df.shape[0]\n",
    "            cols = df.shape[1]\n",
    "            total_rows += rows\n",
    "            print(f\"  - {sheet_name}: {rows:,} rows, {cols} columns\")\n",
    "        \n",
    "        print(f\"\\nCOMPLETE - Total rows across all sheets: {total_rows:,}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— Error: File '{excel_file}' not found.\")\n",
    "        print(\"Please place your Excel file in the data/raw/ directory and update the path above.\")\n",
    "        sheets_dict = {}\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading Excel file: {e}\")\n",
    "        sheets_dict = {}\n",
    "else:\n",
    "    excel_file = None\n",
    "    sheets_dict = {}\n",
    "    print(\"âœ— No Excel files found in ../data/raw/\")\n",
    "    print(\"Please place an Excel workbook (.xlsx) in the data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f78826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sheet: COLA FONL Unique\n",
      "  Found text columns: ['Contact Reason', 'Contact Reason Detail', 'Comment']\n",
      "    - Contact Reason: 2517 texts extracted\n",
      "    - Contact Reason Detail: 2517 texts extracted\n",
      "    - Comment: 2517 texts extracted\n",
      "\n",
      "Processing sheet: General Inqury Unique\n",
      "  Found text columns: ['Contact Reason', 'Contact Reason Detail', 'Comment']\n",
      "    - Contact Reason: 1583 texts extracted\n",
      "    - Contact Reason Detail: 1583 texts extracted\n",
      "    - Comment: 1582 texts extracted\n",
      "\n",
      "Processing sheet: Contact Reason Details\n",
      "  Found text columns: ['Contact Reason', 'Contact Reason Detail', 'Comment']\n",
      "    - Contact Reason: 31879 texts extracted\n",
      "    - Contact Reason Detail: 31879 texts extracted\n",
      "    - Comment: 31865 texts extracted\n",
      "\n",
      "âœ“ Total texts extracted: 107922\n",
      "Sample text: COLA/FONL Issues...\n",
      "âœ“ Processed texts saved to data/processed/extracted_texts.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract qualitative text data\n",
    "# Configure text column names to extract from\n",
    "text_columns = ['Contact Reason', 'Contact Reason Detail', 'Comment']  # Add your column names here\n",
    "\n",
    "all_texts = []\n",
    "text_sources = []  # Track which sheet each text came from\n",
    "\n",
    "if sheets_dict:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "        # Find text columns in this sheet\n",
    "        available_text_cols = [col for col in text_columns if col in df.columns]\n",
    "        \n",
    "        if available_text_cols:\n",
    "            print(f\"  Found text columns: {available_text_cols}\")\n",
    "            \n",
    "            # Extract text from each available column\n",
    "            for col in available_text_cols:\n",
    "                texts = df[col].dropna().astype(str).tolist()\n",
    "                all_texts.extend(texts)\n",
    "                text_sources.extend([f\"{sheet_name}:{col}\"] * len(texts))\n",
    "                print(f\"    - {col}: {len(texts)} texts extracted\")\n",
    "        else:\n",
    "            print(f\"  Warning: No text columns found in '{sheet_name}'. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Total texts extracted: {len(all_texts)}\")\n",
    "    \n",
    "    if all_texts:\n",
    "        print(f\"Sample text: {all_texts[0][:100]}...\")\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_df = pd.DataFrame({\n",
    "            'text': all_texts,\n",
    "            'source': text_sources\n",
    "        })\n",
    "        processed_df.to_csv('../data/processed/extracted_texts.csv', index=False)\n",
    "        print(\"âœ“ Processed texts saved to data/processed/extracted_texts.csv\")\n",
    "    else:\n",
    "        print(\"âœ— No texts were extracted. Please check your column names.\")\n",
    "else:\n",
    "    print(\"No workbook loaded. Please check Step 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a99e6f",
   "metadata": {},
   "source": [
    "# Generate Unique comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d679fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Extracting unique comments from all sheets...\n",
      "  Found 'Comment' column in sheet: COLA FONL Unique\n",
      "    - Extracted 2517 comments\n",
      "  Found 'Comment' column in sheet: General Inqury Unique\n",
      "    - Extracted 1582 comments\n",
      "  Found 'Comment' column in sheet: Contact Reason Details\n",
      "    - Extracted 31865 comments\n",
      "\n",
      "âœ“ Comment extraction complete:\n",
      "  - Total comments extracted: 35964\n",
      "  - Duplicates removed: 11993\n",
      "  - Unique comments: 23971\n",
      "âœ“ Unique comments saved to data/processed/unique_comments.csv\n",
      "Sample comment:  \tADDRESS VERIFICATION FOR PERMIT FL-I-15562...\n"
     ]
    }
   ],
   "source": [
    "# Step 2.25: Extract unique comments from all sheets\n",
    "# This creates a deduplicated list of all comments across the workbook\n",
    "\n",
    "all_comments = []\n",
    "comment_sources = []  # Track which sheet each comment came from\n",
    "\n",
    "if sheets_dict:\n",
    "    print(\"\\nðŸ” Extracting unique comments from all sheets...\")\n",
    "    \n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        # Look for \"Comment\" column (case-insensitive)\n",
    "        comment_cols = [col for col in df.columns if col.lower() == 'comment']\n",
    "        \n",
    "        if comment_cols:\n",
    "            comment_col = comment_cols[0]  # Take the first match\n",
    "            print(f\"  Found 'Comment' column in sheet: {sheet_name}\")\n",
    "            \n",
    "            # Extract comments, remove nulls and empty strings\n",
    "            comments = df[comment_col].dropna().astype(str)\n",
    "            comments = comments[comments.str.strip() != '']  # Remove empty/whitespace-only\n",
    "            \n",
    "            comment_list = comments.tolist()\n",
    "            all_comments.extend(comment_list)\n",
    "            comment_sources.extend([f\"{sheet_name}:{comment_col}\"] * len(comment_list))\n",
    "            \n",
    "            print(f\"    - Extracted {len(comment_list)} comments\")\n",
    "        else:\n",
    "            print(f\"  No 'Comment' column found in sheet: {sheet_name}\")\n",
    "    \n",
    "    if all_comments:\n",
    "        # Create DataFrame and deduplicate\n",
    "        comments_df = pd.DataFrame({\n",
    "            'Comment': all_comments,\n",
    "            'source': comment_sources\n",
    "        })\n",
    "        \n",
    "        # Track duplicates before removing\n",
    "        original_count = len(comments_df)\n",
    "        comments_df = comments_df.drop_duplicates(subset=['Comment'], keep='first')\n",
    "        unique_count = len(comments_df)\n",
    "        duplicates_removed = original_count - unique_count\n",
    "        \n",
    "        print(f\"\\nâœ“ Comment extraction complete:\")\n",
    "        print(f\"  - Total comments extracted: {original_count}\")\n",
    "        print(f\"  - Duplicates removed: {duplicates_removed}\")\n",
    "        print(f\"  - Unique comments: {unique_count}\")\n",
    "        \n",
    "        # Export unique comments (only the Comment column)\n",
    "        unique_comments_df = comments_df[['Comment']].copy()\n",
    "        unique_comments_df.to_csv('../data/processed/unique_comments.csv', index=False)\n",
    "        print(\"âœ“ Unique comments saved to data/processed/unique_comments.csv\")\n",
    "        \n",
    "        # Show sample\n",
    "        if unique_count > 0:\n",
    "            print(f\"Sample comment: {unique_comments_df['Comment'].iloc[0][:100]}...\")\n",
    "    else:\n",
    "        print(\"âœ— No comments found in any sheets with 'Comment' column.\")\n",
    "else:\n",
    "    print(\"No workbook loaded. Please check Step 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a39a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 14:11:16,102 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTOPIC analysis on 107922 texts...\n",
      "Dataset is large (107922 texts). Sampling to 1000 for analysis to avoid memory issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8561eb5373114bc99d4a5983886f3767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba21b24a5334879b963a631fa3d429f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 14:11:18,780 - BERTopic - Embedding - Completed âœ“\n",
      "2026-01-31 14:11:18,780 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-31 14:11:24,213 - BERTopic - Dimensionality - Completed âœ“\n",
      "2026-01-31 14:11:24,214 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-31 14:11:24,221 - BERTopic - Cluster - Completed âœ“\n",
      "2026-01-31 14:11:24,221 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2026-01-31 14:11:24,230 - BERTopic - Representation - Completed âœ“\n",
      "2026-01-31 14:11:24,231 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-31 14:11:24,234 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-31 14:11:24,239 - BERTopic - Representation - Completed âœ“\n",
      "2026-01-31 14:11:24,239 - BERTopic - Topic reduction - Reduced number of topics from 40 to 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ BERTopic identified 16 topics (excluding outliers)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>76</td>\n",
       "      <td>-1_label_to_explained_applicant</td>\n",
       "      <td>[label, to, explained, applicant, on, the, im,...</td>\n",
       "      <td>[IM shared information regarding the needs cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>0_issues_labeling_colafonl_questions</td>\n",
       "      <td>[issues, labeling, colafonl, questions, formul...</td>\n",
       "      <td>[Labeling Questions, Labeling Questions, Label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1_wine_ct_x000d_he</td>\n",
       "      <td>[wine, ct, x000d, he, is, red, puerto, percent...</td>\n",
       "      <td>[Wine, Wine, Wine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2_correction_clarification_needs_ds</td>\n",
       "      <td>[correction, clarification, needs, ds, appeale...</td>\n",
       "      <td>[Needs Correction- Clarification, Needs Correc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>3_transfer_calls_extensions_divisions</td>\n",
       "      <td>[transfer, calls, extensions, divisions, call,...</td>\n",
       "      <td>[Transfer calls to other Extensions or Divisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>4_spirits_distilled__</td>\n",
       "      <td>[spirits, distilled, , , , , , , , ]</td>\n",
       "      <td>[Distilled Spirits, Distilled Spirits, Distill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>5_new_registration_permit_user</td>\n",
       "      <td>[new, registration, permit, user, nrc, iris, a...</td>\n",
       "      <td>[New User Registration, New User Registration,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>6_general_inquiry_answered_sulfte</td>\n",
       "      <td>[general, inquiry, answered, sulfte, answer, n...</td>\n",
       "      <td>[General Inquiry, General Inquiry, General Inq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>7____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[-, -, -]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>8_other___</td>\n",
       "      <td>[other, , , , , , , , , ]</td>\n",
       "      <td>[Other, Other, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>9_malt_beverage_whiskey_bourbon</td>\n",
       "      <td>[malt, beverage, whiskey, bourbon, contract, w...</td>\n",
       "      <td>[Malt Beverage, Malt Beverage, Malt Beverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>10_allowable_revisions_varietal_revision</td>\n",
       "      <td>[allowable, revisions, varietal, revision, gra...</td>\n",
       "      <td>[Allowable Revisions, Allowable Revisions, All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>11_modify_user_registration_modifying</td>\n",
       "      <td>[modify, user, registration, modifying, assist...</td>\n",
       "      <td>[Modify User Registration, Modify User Registr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>12_spirits_distilled_transfer_to</td>\n",
       "      <td>[spirits, distilled, transfer, to, , , , , , ]</td>\n",
       "      <td>[Distilled Spirits, Distilled Spirits, Distill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>13_ingredients_ingredient_x000d_question</td>\n",
       "      <td>[ingredients, ingredient, x000d, question, hon...</td>\n",
       "      <td>[Ingredients, Ingredients, Ingredients]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>14_application_fanciful_qa_the</td>\n",
       "      <td>[application, fanciful, qa, the, approved, on,...</td>\n",
       "      <td>[wrong fanciful name on the approved applicati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                      Name  \\\n",
       "0      -1     76           -1_label_to_explained_applicant   \n",
       "1       0    332      0_issues_labeling_colafonl_questions   \n",
       "2       1    104                        1_wine_ct_x000d_he   \n",
       "3       2     74       2_correction_clarification_needs_ds   \n",
       "4       3     56     3_transfer_calls_extensions_divisions   \n",
       "5       4     54                     4_spirits_distilled__   \n",
       "6       5     54            5_new_registration_permit_user   \n",
       "7       6     38         6_general_inquiry_answered_sulfte   \n",
       "8       7     37                                     7____   \n",
       "9       8     36                                8_other___   \n",
       "10      9     29           9_malt_beverage_whiskey_bourbon   \n",
       "11     10     25  10_allowable_revisions_varietal_revision   \n",
       "12     11     23     11_modify_user_registration_modifying   \n",
       "13     12     20          12_spirits_distilled_transfer_to   \n",
       "14     13     18  13_ingredients_ingredient_x000d_question   \n",
       "15     14     18            14_application_fanciful_qa_the   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [label, to, explained, applicant, on, the, im,...   \n",
       "1   [issues, labeling, colafonl, questions, formul...   \n",
       "2   [wine, ct, x000d, he, is, red, puerto, percent...   \n",
       "3   [correction, clarification, needs, ds, appeale...   \n",
       "4   [transfer, calls, extensions, divisions, call,...   \n",
       "5                [spirits, distilled, , , , , , , , ]   \n",
       "6   [new, registration, permit, user, nrc, iris, a...   \n",
       "7   [general, inquiry, answered, sulfte, answer, n...   \n",
       "8                                [, , , , , , , , , ]   \n",
       "9                           [other, , , , , , , , , ]   \n",
       "10  [malt, beverage, whiskey, bourbon, contract, w...   \n",
       "11  [allowable, revisions, varietal, revision, gra...   \n",
       "12  [modify, user, registration, modifying, assist...   \n",
       "13     [spirits, distilled, transfer, to, , , , , , ]   \n",
       "14  [ingredients, ingredient, x000d, question, hon...   \n",
       "15  [application, fanciful, qa, the, approved, on,...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [IM shared information regarding the needs cor...  \n",
       "1   [Labeling Questions, Labeling Questions, Label...  \n",
       "2                                  [Wine, Wine, Wine]  \n",
       "3   [Needs Correction- Clarification, Needs Correc...  \n",
       "4   [Transfer calls to other Extensions or Divisio...  \n",
       "5   [Distilled Spirits, Distilled Spirits, Distill...  \n",
       "6   [New User Registration, New User Registration,...  \n",
       "7   [General Inquiry, General Inquiry, General Inq...  \n",
       "8                                           [-, -, -]  \n",
       "9                               [Other, Other, Other]  \n",
       "10      [Malt Beverage, Malt Beverage, Malt Beverage]  \n",
       "11  [Allowable Revisions, Allowable Revisions, All...  \n",
       "12  [Modify User Registration, Modify User Registr...  \n",
       "13  [Distilled Spirits, Distilled Spirits, Distill...  \n",
       "14            [Ingredients, Ingredients, Ingredients]  \n",
       "15  [wrong fanciful name on the approved applicati...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Apply BERTopic for theme extraction\n",
    "model_choice = globals().get('model_choice', 'bertopic')\n",
    "# Set default sample size for large datasets\n",
    "sample_size = globals().get('sample_size', 1000)\n",
    "\n",
    "if all_texts:\n",
    "    print(f\"Running {model_choice.upper()} analysis on {len(all_texts)} texts...\")\n",
    "    \n",
    "    # Handle large datasets by sampling\n",
    "    if len(all_texts) > 50000:\n",
    "        print(f\"Dataset is large ({len(all_texts)} texts). Sampling to {sample_size} for analysis to avoid memory issues.\")\n",
    "        sample_indices = np.random.choice(len(all_texts), sample_size, replace=False)\n",
    "        analysis_texts = [all_texts[i] for i in sample_indices]\n",
    "        analysis_sources = [text_sources[i] for i in sample_indices]\n",
    "    else:\n",
    "        analysis_texts = all_texts\n",
    "        analysis_sources = text_sources\n",
    "\n",
    "    # Import additional libraries for custom models\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    \n",
    "    # Initialize BERTopic model with optimized settings for large datasets\n",
    "    topic_model = BERTopic(\n",
    "        language=\"english\",\n",
    "        # calculate_probabilities=False,  # Set to False to reduce memory usage\n",
    "        verbose=True,\n",
    "        min_topic_size=10,  # Increased from 5 to reduce number of topics and memory\n",
    "        nr_topics=\"auto\",   # Auto-determine number of topics\n",
    "        low_memory=True,  # Enable low memory mode for large datasets\n",
    "        umap_model=UMAP(n_jobs=1),  # Disable multiprocessing in UMAP\n",
    "        hdbscan_model=HDBSCAN(core_dist_n_jobs=1)  # Disable multiprocessing in HDBSCAN\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    topics, probabilities = topic_model.fit_transform(analysis_texts)\n",
    "    \n",
    "    # Get topic information\n",
    "    theme_info = topic_model.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nâœ“ BERTopic identified {len(theme_info)-1} topics (excluding outliers)\")\n",
    "    \n",
    "    # Display extracted themes\n",
    "    display(theme_info.head(len(theme_info)-1))\n",
    "    \n",
    "else:\n",
    "    print(\"No texts available for analysis. Please check Step 2.\")\n",
    "    theme_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44012421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize topics\n",
    "# Safely get variables that may not exist in the current kernel/session\n",
    "# theme_info = globals().get('theme_info', None)\n",
    "# model_choice = globals().get('model_choice', 'bertopic')\n",
    "# topic_model = globals().get('topic_model', None)\n",
    "# analysis_texts = globals().get('analysis_texts', [])\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import plotly.io as pio\n",
    "\n",
    "# CRITICAL: Use 'json' renderer to prevent auto-display blocking\n",
    "# This allows manual HTML embedding via _display_fig() without hanging\n",
    "try:\n",
    "    pio.renderers.default = 'json'\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _display_fig(fig, title=None):\n",
    "    \"\"\"Embed a Plotly figure as HTML to avoid renderer blocking issues.\"\"\"\n",
    "    try:\n",
    "        html = fig.to_html(full_html=False, include_plotlyjs='cdn')\n",
    "        if title:\n",
    "            display(HTML(f\"<h4>{title}</h4>\"))\n",
    "        display(HTML(html))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to render figure as HTML: {e}\")\n",
    "\n",
    "if theme_info is not None and topic_model is not None:\n",
    "    print(\"Generating visualization...\")\n",
    "    print(f\"Dataset size: {len(analysis_texts)} texts, {len(theme_info)} topics\")\n",
    "    \n",
    "    # Topic distribution bar chart - fast and informative\n",
    "    try:\n",
    "        print(\"\\nGenerating topic barchart...\")\n",
    "        fig_barchart = topic_model.visualize_barchart(top_n_topics=15)\n",
    "        _display_fig(fig_barchart, title='Top 15 Topics by Frequency')\n",
    "        print(\"\\nâœ“ Visualization complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Barchart visualization failed: {e}\")\n",
    "    \n",
    "else:\n",
    "    if theme_info is None:\n",
    "        print(\"No themes available for visualization. Run the analysis cells first.\")\n",
    "    elif model_choice == 'bertopic' and topic_model is None:\n",
    "        print(\"BERTopic model object not found in the session. Re-run the BERTopic analysis cell.\")\n",
    "    else:\n",
    "        print(\"No visualizations available for the current configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Export themes for validation/import into QDA tool\n",
    "if theme_info is not None:\n",
    "    # Export to CSV\n",
    "    output_file = f\"../data/results/extracted_themes_{model_choice}.csv\"\n",
    "    theme_info.to_csv(output_file, index=False)\n",
    "    print(f\"âœ“ Themes exported to '{output_file}'\")\n",
    "    \n",
    "    # Export detailed topic information\n",
    "    # Get topic representations\n",
    "    topic_representations = {}\n",
    "    for topic_id in theme_info['Topic'].unique():\n",
    "        if topic_id != -1:  # Skip outlier topic\n",
    "            words = topic_model.get_topic(topic_id)\n",
    "            topic_representations[topic_id] = words\n",
    "    \n",
    "    # Create detailed export\n",
    "    detailed_themes = []\n",
    "    for topic_id, words in topic_representations.items():\n",
    "        detailed_themes.append({\n",
    "            'topic_id': topic_id,\n",
    "            'topic_name': f\"Topic_{topic_id}\",\n",
    "            'top_words': ', '.join([word for word, _ in words[:10]]),\n",
    "            'word_scores': ', '.join([f\"{word}:{score:.3f}\" for word, score in words[:10]]),\n",
    "            'document_count': theme_info[theme_info['Topic'] == topic_id]['Count'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    detailed_df = pd.DataFrame(detailed_themes)\n",
    "    detailed_output = f\"../data/results/detailed_themes_{model_choice}.csv\"\n",
    "    detailed_df.to_csv(detailed_output, index=False)\n",
    "    print(f\"âœ“ Detailed themes exported to '{detailed_output}'\")\n",
    "    \n",
    "    # Export text-topic assignments for QDA validation\n",
    "    text_topics_df = pd.DataFrame({\n",
    "        'text': analysis_texts,\n",
    "        'source': analysis_sources,\n",
    "        'topic': topics,\n",
    "        'topic_probability': [None] * len(topics)  # Probabilities disabled for memory efficiency\n",
    "    })\n",
    "\n",
    "    \n",
    "    assignments_file = f\"../data/results/text_topic_assignments_{model_choice}.csv\"\n",
    "    text_topics_df.to_csv(assignments_file, index=False)\n",
    "    print(f\"âœ“ Text-topic assignments exported to '{assignments_file}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPORT COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Files ready for import into QDA tools like:\")\n",
    "    print(\"- Taguette (import CSV files)\")\n",
    "    print(\"- QualCoder (import CSV files)\")\n",
    "    print(\"- NVivo (import CSV files)\")\n",
    "    print(\"- ATLAS.ti (import CSV files)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No themes to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b682b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Validate AI-generated themes** in your QDA tool\n",
    "2. **Refine topics** by merging, splitting, or renaming as needed\n",
    "3. **Code additional texts** using the validated themes\n",
    "4. **Export coded data** for further quantitative analysis\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "- **Preprocessing**: Clean your text data before analysis (remove noise, standardize formats)\n",
    "- **Model Tuning**: Adjust `min_topic_size` in BERTopic for more/fewer topics\n",
    "- **Language**: Set the correct language if your texts aren't in English\n",
    "- **Scale**: For large datasets (>10k texts), consider running on Databricks Community Edition\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Memory issues**: Reduce `min_topic_size` or use Top2Vec instead\n",
    "- **Poor topics**: Preprocess text better or try different model settings\n",
    "- **No texts found**: Check column names in your Excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdalocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
