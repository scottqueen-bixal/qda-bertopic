{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9770c0bf",
   "metadata": {},
   "source": [
    "# AI-Enhanced Qualitative Data Analysis\n",
    "\n",
    "This notebook processes Excel workbooks containing qualitative data and uses AI models (BERTopic or Top2Vec) to automatically extract themes and topics for qualitative data analysis (QDA).\n",
    "\n",
    "## Features\n",
    "- Load multiple Excel sheets/tabs\n",
    "- Extract qualitative text from specified columns\n",
    "- Apply BERTopic for hierarchical theme extraction\n",
    "- Interactive visualizations\n",
    "- Export results for QDA tools (Taguette, QualCoder, etc.)\n",
    "\n",
    "## Setup\n",
    "1. Place your Excel workbook in the `data/raw/` directory\n",
    "2. Update the `excel_file` path below\n",
    "3. Customize column names if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c753fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "# Uncomment the line below if running in an environment without these packages\n",
    "# !uv install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl  # Faster for large files than pandas default\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import plotly.io as pio\n",
    "import glob\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n",
    "                        gen_min_span_tree=True,\n",
    "                        prediction_data=True)\n",
    "\n",
    "stopwords = list(stopwords.words('english')) + ['to', 'the', 'im', 'for', 'on']\n",
    "\n",
    "# we add this to remove stopwords that can pollute topcs\n",
    "vectorizer_model = CountVectorizer(\n",
    "  ngram_range=(1, 2), \n",
    "  stop_words=stopwords\n",
    ")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Excel workbook\n",
    "# Replace 'your_workbook.xlsx' with the path to your Excel file\n",
    "# Find the first .xlsx file in the data/raw directory\n",
    "excel_files = glob.glob('../data/raw/*.xlsx')\n",
    "\n",
    "if excel_files:\n",
    "    excel_file = excel_files[0]  # Use the first Excel file found\n",
    "    print(f\"Found Excel file: {excel_file}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading workbook (this may take a moment for large files)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use openpyxl engine for better performance on large files\n",
    "        # Load all sheets into a dictionary of DataFrames\n",
    "        sheets_dict = pd.read_excel(excel_file, sheet_name=None, engine='openpyxl')\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"âœ“ Successfully loaded workbook in {load_time:.1f} seconds\")\n",
    "        print(f\"Loaded {len(sheets_dict)} sheets: {list(sheets_dict.keys())}\")\n",
    "        \n",
    "        # Display basic info about each sheet\n",
    "        total_rows = 0\n",
    "        for sheet_name, df in sheets_dict.items():\n",
    "            rows = df.shape[0]\n",
    "            cols = df.shape[1]\n",
    "            total_rows += rows\n",
    "            print(f\"  - {sheet_name}: {rows:,} rows, {cols} columns\")\n",
    "        \n",
    "        print(f\"\\nCOMPLETE - Total rows across all sheets: {total_rows:,}\")\n",
    "        \n",
    "        # Optional: For very large files, load sheets on-demand later\n",
    "        # Instead of loading all at once, you could do:\n",
    "        # xl = pd.ExcelFile(excel_file, engine='openpyxl')\n",
    "        # Then load specific sheets in Step 2: df = xl.parse(sheet_name)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— Error: File '{excel_file}' not found.\")\n",
    "        print(\"Please place your Excel file in the data/raw/ directory and update the path above.\")\n",
    "        sheets_dict = {}\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading Excel file: {e}\")\n",
    "        sheets_dict = {}\n",
    "else:\n",
    "    excel_file = None\n",
    "    sheets_dict = {}\n",
    "    print(\"âœ— No Excel files found in ../data/raw/\")\n",
    "    print(\"Please place an Excel workbook (.xlsx) in the data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f78826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract qualitative text data\n",
    "# Configure text column names to extract from\n",
    "text_columns = ['Contact Reason', 'Contact Reason Detail', 'Comment']  # Add your column names here\n",
    "\n",
    "all_texts = []\n",
    "text_sources = []  # Track which sheet each text came from\n",
    "\n",
    "if sheets_dict:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "        # Find text columns in this sheet\n",
    "        available_text_cols = [col for col in text_columns if col in df.columns]\n",
    "        \n",
    "        if available_text_cols:\n",
    "            print(f\"  Found text columns: {available_text_cols}\")\n",
    "            \n",
    "            # Extract text from each available column\n",
    "            for col in available_text_cols:\n",
    "                texts = df[col].dropna().astype(str).tolist()\n",
    "                all_texts.extend(texts)\n",
    "                text_sources.extend([f\"{sheet_name}:{col}\"] * len(texts))\n",
    "                print(f\"    - {col}: {len(texts)} texts extracted\")\n",
    "        else:\n",
    "            print(f\"  Warning: No text columns found in '{sheet_name}'. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Total texts extracted: {len(all_texts)}\")\n",
    "    \n",
    "    if all_texts:\n",
    "        print(f\"Sample text: {all_texts[0][:100]}...\")\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_df = pd.DataFrame({\n",
    "            'text': all_texts,\n",
    "            'source': text_sources\n",
    "        })\n",
    "        processed_df.to_csv('../data/processed/extracted_texts.csv', index=False)\n",
    "        print(\"âœ“ Processed texts saved to data/processed/extracted_texts.csv\")\n",
    "    else:\n",
    "        print(\"âœ— No texts were extracted. Please check your column names.\")\n",
    "else:\n",
    "    print(\"No workbook loaded. Please check Step 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a99e6f",
   "metadata": {},
   "source": [
    "# Generate Unique comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.25: Extract unique comments from all sheets\n",
    "# This creates a deduplicated list of all comments across the workbook\n",
    "\n",
    "all_comments = []\n",
    "comment_sources = []  # Track which sheet each comment came from\n",
    "\n",
    "if sheets_dict:\n",
    "    print(\"\\nðŸ” Extracting unique comments from all sheets...\")\n",
    "    \n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        # Look for \"Comment\" column (case-insensitive)\n",
    "        comment_cols = [col for col in df.columns if col.lower() == 'comment']\n",
    "        \n",
    "        if comment_cols:\n",
    "            comment_col = comment_cols[0]  # Take the first match\n",
    "            print(f\"  Found 'Comment' column in sheet: {sheet_name}\")\n",
    "            \n",
    "            # Extract comments, remove nulls and empty strings\n",
    "            comments = df[comment_col].dropna().astype(str)\n",
    "            comments = comments[comments.str.strip() != '']  # Remove empty/whitespace-only\n",
    "            \n",
    "            comment_list = comments.tolist()\n",
    "            all_comments.extend(comment_list)\n",
    "            comment_sources.extend([f\"{sheet_name}:{comment_col}\"] * len(comment_list))\n",
    "            \n",
    "            print(f\"    - Extracted {len(comment_list)} comments\")\n",
    "        else:\n",
    "            print(f\"  No 'Comment' column found in sheet: {sheet_name}\")\n",
    "    \n",
    "    if all_comments:\n",
    "        # Create DataFrame and deduplicate\n",
    "        comments_df = pd.DataFrame({\n",
    "            'Comment': all_comments,\n",
    "            'source': comment_sources\n",
    "        })\n",
    "        \n",
    "        # Track duplicates before removing\n",
    "        original_count = len(comments_df)\n",
    "        comments_df = comments_df.drop_duplicates(subset=['Comment'], keep='first')\n",
    "        unique_count = len(comments_df)\n",
    "        duplicates_removed = original_count - unique_count\n",
    "        \n",
    "        print(f\"\\nâœ“ Comment extraction complete:\")\n",
    "        print(f\"  - Total comments extracted: {original_count}\")\n",
    "        print(f\"  - Duplicates removed: {duplicates_removed}\")\n",
    "        print(f\"  - Unique comments: {unique_count}\")\n",
    "        \n",
    "        # Export unique comments (only the Comment column)\n",
    "        unique_comments_df = comments_df[['Comment']].copy()\n",
    "        unique_comments_df.to_csv('../data/processed/unique_comments.csv', index=False)\n",
    "        print(\"âœ“ Unique comments saved to data/processed/unique_comments.csv\")\n",
    "        \n",
    "        # Show sample\n",
    "        if unique_count > 0:\n",
    "            print(f\"Sample comment: {unique_comments_df['Comment'].iloc[0][:100]}...\")\n",
    "    else:\n",
    "        print(\"âœ— No comments found in any sheets with 'Comment' column.\")\n",
    "else:\n",
    "    print(\"No workbook loaded. Please check Step 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply BERTopic for theme extraction\n",
    "model_choice = globals().get('model_choice', 'bertopic')\n",
    "sampling_max = 50000\n",
    "# Set default sample size for large datasets\n",
    "sample_size = globals().get('sample_size', 40000)\n",
    "\n",
    "if all_texts:\n",
    "    print(f\"Running {model_choice.upper()} analysis on {len(all_texts)} texts...\")\n",
    "    \n",
    "    # Handle large datasets by sampling\n",
    "    if len(all_texts) > sampling_max:\n",
    "        print(f\"Dataset is large ({len(all_texts)} texts). Sampling to {sample_size} for analysis to avoid memory issues.\")\n",
    "        sample_indices = np.random.choice(len(all_texts), sample_size, replace=False)\n",
    "        analysis_texts = [all_texts[i] for i in sample_indices]\n",
    "        analysis_sources = [text_sources[i] for i in sample_indices]\n",
    "    else:\n",
    "        analysis_texts = all_texts\n",
    "        analysis_sources = text_sources\n",
    "    \n",
    "    # Initialize BERTopic model with optimized settings for large datasets\n",
    "    topic_model = BERTopic(\n",
    "        language=\"english\",\n",
    "        calculate_probabilities=True,  # Set to False to reduce memory usage\n",
    "        verbose=True,\n",
    "        min_topic_size=20,  # Increased from 5 to reduce number of topics and memory\n",
    "        top_n_words=5,\n",
    "        nr_topics=\"auto\",   # Auto-determine number of topics\n",
    "        low_memory=True,  # Enable low memory mode for large datasets\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        # umap_model=UMAP(n_jobs=1),  # Disable multiprocessing in UMAP\n",
    "        # hdbscan_model=HDBSCAN(core_dist_n_jobs=1)  # Disable multiprocessing in HDBSCAN\n",
    "        embedding_model=embedding_model,\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    topics, probabilities = topic_model.fit_transform(analysis_texts)\n",
    "    \n",
    "    # Get topic information\n",
    "    theme_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Set custom topic labels for better visualization\n",
    "    topic_labels = {}\n",
    "    for _, row in theme_info.iterrows():\n",
    "        if row['Topic'] != -1:\n",
    "            topic_labels[row['Topic']] = row['Name']\n",
    "    topic_model.set_topic_labels(topic_labels)\n",
    "    \n",
    "\n",
    "    print(f\"\\nâœ“ BERTopic identified {len(theme_info)-1} topics (excluding outliers)\")\n",
    "    \n",
    "    # Display extracted themes\n",
    "    display(theme_info.head(len(theme_info)-1))\n",
    "    \n",
    "else:\n",
    "    print(\"No texts available for analysis. Please check Step 2.\")\n",
    "    theme_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44012421",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "# Step 4: Visualize topics\n",
    "\n",
    "# CRITICAL: Use 'json' renderer to prevent auto-display blocking\n",
    "# This allows manual HTML embedding via _display_fig() without hanging\n",
    "try:\n",
    "    pio.renderers.default = 'json'\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _display_fig(fig, title=None):\n",
    "    \"\"\"Embed a Plotly figure as HTML to avoid renderer blocking issues.\"\"\"\n",
    "    try:\n",
    "        html = fig.to_html(full_html=False, include_plotlyjs='cdn')\n",
    "        if title:\n",
    "            display(HTML(f\"<h4>{title}</h4>\"))\n",
    "        display(HTML(html))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to render figure as HTML: {e}\")\n",
    "\n",
    "if theme_info is not None and topic_model is not None:\n",
    "    print(\"Generating visualization...\")\n",
    "    print(f\"Dataset size: {len(analysis_texts)} texts, {len(theme_info)} topics\")\n",
    "    \n",
    "    # Topic distribution bar chart - fast and informative\n",
    "    try:\n",
    "        print(\"\\nGenerating topic barchart...\")\n",
    "        fig_barchart = topic_model.visualize_barchart(top_n_topics=10)\n",
    "        # The numbers on the x-axis represent the c-TF-IDF \n",
    "        # (class-based Term Frequency-Inverse Document Frequency) \n",
    "        # scores for the top words in each topic. Higher scores indicate greater \n",
    "        # importance of the word within that specific topic.\n",
    "        _display_fig(fig_barchart, title='Top 10 Topics by Frequency')\n",
    "        print(\"\\nâœ“ Visualization complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Barchart visualization failed: {e}\")\n",
    "    \n",
    "else:\n",
    "    if theme_info is None:\n",
    "        print(\"No themes available for visualization. Run the analysis cells first.\")\n",
    "    elif model_choice == 'bertopic' and topic_model is None:\n",
    "        print(\"BERTopic model object not found in the session. Re-run the BERTopic analysis cell.\")\n",
    "    else:\n",
    "        print(\"No visualizations available for the current configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Export themes for validation/import into QDA tool\n",
    "if theme_info is not None:\n",
    "    # Export to CSV\n",
    "    output_file = f\"../data/results/extracted_themes_{model_choice}.csv\"\n",
    "    theme_info.to_csv(output_file, index=False)\n",
    "    print(f\"âœ“ Themes exported to '{output_file}'\")\n",
    "    \n",
    "    # Export detailed topic information\n",
    "    # Get topic representations\n",
    "    topic_representations = {}\n",
    "    for topic_id in theme_info['Topic'].unique():\n",
    "        if topic_id != -1:  # Skip outlier topic\n",
    "            words = topic_model.get_topic(topic_id)\n",
    "            topic_representations[topic_id] = words\n",
    "    \n",
    "    # Create detailed export\n",
    "    detailed_themes = []\n",
    "    for topic_id, words in topic_representations.items():\n",
    "        detailed_themes.append({\n",
    "            'topic_id': topic_id,\n",
    "            'topic_name': f\"Topic_{topic_id}\",\n",
    "            'top_words': ', '.join([word for word, _ in words[:10]]),\n",
    "            'word_scores': ', '.join([f\"{word}:{score:.3f}\" for word, score in words[:10]]),\n",
    "            'document_count': theme_info[theme_info['Topic'] == topic_id]['Count'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    detailed_df = pd.DataFrame(detailed_themes)\n",
    "    detailed_output = f\"../data/results/detailed_themes_{model_choice}.csv\"\n",
    "    detailed_df.to_csv(detailed_output, index=False)\n",
    "    print(f\"âœ“ Detailed themes exported to '{detailed_output}'\")\n",
    "    \n",
    "    # Export text-topic assignments for QDA validation\n",
    "    text_topics_df = pd.DataFrame({\n",
    "        'text': analysis_texts,\n",
    "        'source': analysis_sources,\n",
    "        'topic': topics,\n",
    "        'topic_probability': [None] * len(topics)  # Probabilities disabled for memory efficiency\n",
    "    })\n",
    "\n",
    "    \n",
    "    assignments_file = f\"../data/results/text_topic_assignments_{model_choice}.csv\"\n",
    "    text_topics_df.to_csv(assignments_file, index=False)\n",
    "    print(f\"âœ“ Text-topic assignments exported to '{assignments_file}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPORT COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Files ready for import into QDA tools like:\")\n",
    "    print(\"- Taguette (import CSV files)\")\n",
    "    print(\"- QualCoder (import CSV files)\")\n",
    "    print(\"- NVivo (import CSV files)\")\n",
    "    print(\"- ATLAS.ti (import CSV files)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No themes to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b682b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Validate AI-generated themes** in your QDA tool\n",
    "2. **Refine topics** by merging, splitting, or renaming as needed\n",
    "3. **Code additional texts** using the validated themes\n",
    "4. **Export coded data** for further quantitative analysis\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "- **Preprocessing**: Clean your text data before analysis (remove noise, standardize formats)\n",
    "- **Model Tuning**: Adjust `min_topic_size` in BERTopic for more/fewer topics\n",
    "- **Language**: Set the correct language if your texts aren't in English\n",
    "- **Scale**: For large datasets (>10k texts), consider running on Databricks Community Edition\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Memory issues**: Reduce `min_topic_size` or use Top2Vec instead\n",
    "- **Poor topics**: Preprocess text better or try different model settings\n",
    "- **No texts found**: Check column names in your Excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
