{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9770c0bf",
   "metadata": {},
   "source": [
    "# AI-Enhanced Qualitative Data Analysis\n",
    "\n",
    "This notebook processes Excel workbooks containing qualitative data and uses AI models (BERTopic or Top2Vec) to automatically extract themes and topics for qualitative data analysis (QDA).\n",
    "\n",
    "## Features\n",
    "- Load multiple Excel sheets/tabs\n",
    "- Extract qualitative text from specified columns\n",
    "- Apply BERTopic for hierarchical theme extraction\n",
    "- Alternative Top2Vec implementation\n",
    "- Interactive visualizations\n",
    "- Export results for QDA tools (Taguette, QualCoder, etc.)\n",
    "\n",
    "## Setup\n",
    "1. Place your Excel workbook in the `data/raw/` directory\n",
    "2. Update the `excel_file` path below\n",
    "3. Customize column names if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c753fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "# Uncomment the line below if running in an environment without these packages\n",
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa16acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280f4cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded workbook: ../data/raw/callcenterdata.xlsx\n",
      "Loaded sheets: ['COLA FONL Unique', 'General Inqury Unique', 'Unique Values', 'Contact Reason Details', 'Summary', 'Filtered Patty', 'Further anlaysis']\n",
      "  - COLA FONL Unique: 2517 rows, 11 columns\n",
      "  - General Inqury Unique: 1583 rows, 11 columns\n",
      "  - Unique Values: 57974 rows, 4 columns\n",
      "  - Contact Reason Details: 31879 rows, 11 columns\n",
      "  - Summary: 247 rows, 4 columns\n",
      "  - Filtered Patty: 249 rows, 10 columns\n",
      "  - Further anlaysis: 8 rows, 16 columns\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the Excel workbook\n",
    "# Replace 'your_workbook.xlsx' with the path to your Excel file\n",
    "excel_file = '../data/raw/callcenterdata.xlsx'\n",
    "\n",
    "# Load all sheets into a dictionary of DataFrames\n",
    "try:\n",
    "    sheets_dict = pd.read_excel(excel_file, sheet_name=None)\n",
    "    print(f\"✓ Successfully loaded workbook: {excel_file}\")\n",
    "    print(f\"Loaded sheets: {list(sheets_dict.keys())}\")\n",
    "    \n",
    "    # Display basic info about each sheet\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        print(f\"  - {sheet_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ Error: File '{excel_file}' not found.\")\n",
    "    print(\"Please place your Excel file in the data/raw/ directory and update the path above.\")\n",
    "    sheets_dict = {}\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading Excel file: {e}\")\n",
    "    sheets_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f78826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sheet: COLA FONL Unique\n",
      "  Found text columns: ['Comment']\n",
      "    - Comment: 2517 texts extracted\n",
      "\n",
      "Processing sheet: General Inqury Unique\n",
      "  Found text columns: ['Comment']\n",
      "    - Comment: 1582 texts extracted\n",
      "\n",
      "Processing sheet: Unique Values\n",
      "  Found text columns: ['Comment']\n",
      "    - Comment: 57944 texts extracted\n",
      "\n",
      "Processing sheet: Contact Reason Details\n",
      "  Found text columns: ['Comment']\n",
      "    - Comment: 31865 texts extracted\n",
      "\n",
      "Processing sheet: Summary\n",
      "  Warning: No text columns found in 'Summary'. Available columns: ['Date Range: 10/1/2021 to 12/10/2025', 'Unnamed: 1', 'Unnamed: 2', 'Instance']\n",
      "\n",
      "Processing sheet: Filtered Patty\n",
      "  Warning: No text columns found in 'Filtered Patty'. Available columns: ['Date Range: 10/1/2021 to 12/10/2025', 'Contact Reason Detail', 'Instance', 'Unnamed: 3', 'Sorted by type', 'Unnamed: 5', 'All', 'Unnamed: 7', 'All - collapsed', 'Where']\n",
      "\n",
      "Processing sheet: Further anlaysis\n",
      "  Warning: No text columns found in 'Further anlaysis'. Available columns: ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Wine top reasons for contact', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Formulation* top reasons for contact', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Distilled spirits top reasons for contact', 'Unnamed: 14', 'Unnamed: 15']\n",
      "\n",
      "✓ Total texts extracted: 93908\n",
      "Sample text:  \tADDRESS VERIFICATION FOR PERMIT FL-I-15562...\n",
      "✓ Processed texts saved to data/processed/extracted_texts.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract qualitative text data\n",
    "# Configure text column names to extract from\n",
    "text_columns = ['Comment']  # Add your column names here\n",
    "\n",
    "all_texts = []\n",
    "text_sources = []  # Track which sheet each text came from\n",
    "\n",
    "if sheets_dict:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "        # Find text columns in this sheet\n",
    "        available_text_cols = [col for col in text_columns if col in df.columns]\n",
    "        \n",
    "        if available_text_cols:\n",
    "            print(f\"  Found text columns: {available_text_cols}\")\n",
    "            \n",
    "            # Extract text from each available column\n",
    "            for col in available_text_cols:\n",
    "                texts = df[col].dropna().astype(str).tolist()\n",
    "                all_texts.extend(texts)\n",
    "                text_sources.extend([f\"{sheet_name}:{col}\"] * len(texts))\n",
    "                print(f\"    - {col}: {len(texts)} texts extracted\")\n",
    "        else:\n",
    "            print(f\"  Warning: No text columns found in '{sheet_name}'. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\n✓ Total texts extracted: {len(all_texts)}\")\n",
    "    \n",
    "    if all_texts:\n",
    "        print(f\"Sample text: {all_texts[0][:100]}...\")\n",
    "        \n",
    "        # Save processed data\n",
    "        processed_df = pd.DataFrame({\n",
    "            'text': all_texts,\n",
    "            'source': text_sources\n",
    "        })\n",
    "        processed_df.to_csv('../data/processed/extracted_texts.csv', index=False)\n",
    "        print(\"✓ Processed texts saved to data/processed/extracted_texts.csv\")\n",
    "    else:\n",
    "        print(\"✗ No texts were extracted. Please check your column names.\")\n",
    "else:\n",
    "    print(\"No workbook loaded. Please check Step 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a39a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 14:57:17,899 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTOPIC analysis on 93908 texts...\n",
      "Dataset is large (93908 texts). Sampling to 50,000 for analysis to avoid memory issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff90811f588c47809887197bf968979a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ecc32d1d88486c8a93e4868ae5a166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 14:57:46,935 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-28 14:57:46,935 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-28 14:58:27,489 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-28 14:58:27,490 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-28 14:58:28,068 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-28 14:58:28,068 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2026-01-28 14:58:28,525 - BERTopic - Representation - Completed ✓\n",
      "2026-01-28 14:58:28,526 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-28 14:58:31,014 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-28 14:58:31,483 - BERTopic - Representation - Completed ✓\n",
      "2026-01-28 14:58:31,487 - BERTopic - Topic reduction - Reduced number of topics from 2256 to 1262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ BERTopic identified 1261 topics (excluding outliers)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8311</td>\n",
       "      <td>-1_x000d_and_product_alcohol</td>\n",
       "      <td>[x000d, and, product, alcohol, is, that, be, i...</td>\n",
       "      <td>[Caller asked general questions about malt bev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>538</td>\n",
       "      <td>0_ttb_return_x000d_id</td>\n",
       "      <td>[ttb, return, x000d, id, fedex, locate, 243410...</td>\n",
       "      <td>[TTB#23065001001026, TTB#25213001000382, TTB#2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>515</td>\n",
       "      <td>1_hang_hangup_up_hung</td>\n",
       "      <td>[hang, hangup, up, hung, action, na, hange, ha...</td>\n",
       "      <td>[hang up, hang up, Hang up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>418</td>\n",
       "      <td>2_ds_labeling_questions_with</td>\n",
       "      <td>[ds, labeling, questions, with, im, quesstions...</td>\n",
       "      <td>[im with ds labeling questions, im with ds lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>3_ds_dss_rakija_bam</td>\n",
       "      <td>[ds, dss, rakija, bam, dis, ws, arak, canadian...</td>\n",
       "      <td>[DS, DS, DS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>376</td>\n",
       "      <td>4_revision_allowable_revisions_10</td>\n",
       "      <td>[revision, allowable, revisions, 10, list, net...</td>\n",
       "      <td>[Referred allowable revision# 3(d), Allowable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>369</td>\n",
       "      <td>5_wine_os_cooking_frutis</td>\n",
       "      <td>[wine, os, cooking, frutis, bootcamp, 1wine, d...</td>\n",
       "      <td>[WINE, WINE, WINE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>362</td>\n",
       "      <td>6_vm_asmw_surrender_applications</td>\n",
       "      <td>[vm, asmw, surrender, applications, appellatio...</td>\n",
       "      <td>[vm, VM, VM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>355</td>\n",
       "      <td>7_fonl_transferred_inquire_specific</td>\n",
       "      <td>[fonl, transferred, inquire, specific, rejecte...</td>\n",
       "      <td>[transferred to FONL, transferred to FONL, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>320</td>\n",
       "      <td>8_bring_compliance_into_how</td>\n",
       "      <td>[bring, compliance, into, how, explained, labe...</td>\n",
       "      <td>[Explained how to bring the label into complia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                 Name  \\\n",
       "0     -1   8311         -1_x000d_and_product_alcohol   \n",
       "1      0    538                0_ttb_return_x000d_id   \n",
       "2      1    515                1_hang_hangup_up_hung   \n",
       "3      2    418         2_ds_labeling_questions_with   \n",
       "4      3    379                  3_ds_dss_rakija_bam   \n",
       "5      4    376    4_revision_allowable_revisions_10   \n",
       "6      5    369             5_wine_os_cooking_frutis   \n",
       "7      6    362     6_vm_asmw_surrender_applications   \n",
       "8      7    355  7_fonl_transferred_inquire_specific   \n",
       "9      8    320          8_bring_compliance_into_how   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [x000d, and, product, alcohol, is, that, be, i...   \n",
       "1  [ttb, return, x000d, id, fedex, locate, 243410...   \n",
       "2  [hang, hangup, up, hung, action, na, hange, ha...   \n",
       "3  [ds, labeling, questions, with, im, quesstions...   \n",
       "4  [ds, dss, rakija, bam, dis, ws, arak, canadian...   \n",
       "5  [revision, allowable, revisions, 10, list, net...   \n",
       "6  [wine, os, cooking, frutis, bootcamp, 1wine, d...   \n",
       "7  [vm, asmw, surrender, applications, appellatio...   \n",
       "8  [fonl, transferred, inquire, specific, rejecte...   \n",
       "9  [bring, compliance, into, how, explained, labe...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Caller asked general questions about malt bev...  \n",
       "1  [TTB#23065001001026, TTB#25213001000382, TTB#2...  \n",
       "2                        [hang up, hang up, Hang up]  \n",
       "3  [im with ds labeling questions, im with ds lab...  \n",
       "4                                       [DS, DS, DS]  \n",
       "5  [Referred allowable revision# 3(d), Allowable ...  \n",
       "6                                 [WINE, WINE, WINE]  \n",
       "7                                       [vm, VM, VM]  \n",
       "8  [transferred to FONL, transferred to FONL, tra...  \n",
       "9  [Explained how to bring the label into complia...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Apply BERTopic for theme extraction\n",
    "# Select model to run ('bertopic' or 'top2vec') -- uses existing value if set\n",
    "model_choice = globals().get('model_choice', 'bertopic')\n",
    "\n",
    "if all_texts:\n",
    "    print(f\"Running {model_choice.upper()} analysis on {len(all_texts)} texts...\")\n",
    "    \n",
    "    # Handle large datasets by sampling\n",
    "    if len(all_texts) > 50000:\n",
    "        print(f\"Dataset is large ({len(all_texts)} texts). Sampling to 50,000 for analysis to avoid memory issues.\")\n",
    "        sample_indices = np.random.choice(len(all_texts), 50000, replace=False)\n",
    "        analysis_texts = [all_texts[i] for i in sample_indices]\n",
    "        analysis_sources = [text_sources[i] for i in sample_indices]\n",
    "    else:\n",
    "        analysis_texts = all_texts\n",
    "        analysis_sources = text_sources\n",
    "    \n",
    "    if model_choice == 'bertopic':\n",
    "        # Import additional libraries for custom models\n",
    "        from umap import UMAP\n",
    "        from hdbscan import HDBSCAN\n",
    "        \n",
    "        # Initialize BERTopic model with optimized settings for large datasets\n",
    "        topic_model = BERTopic(\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=False,  # Set to False to reduce memory usage\n",
    "            verbose=True,\n",
    "            min_topic_size=5,  # Increased from 5 to reduce number of topics and memory\n",
    "            nr_topics=\"auto\",   # Auto-determine number of topics\n",
    "            low_memory=True,  # Enable low memory mode for large datasets\n",
    "            umap_model=UMAP(n_jobs=1),  # Disable multiprocessing in UMAP\n",
    "            hdbscan_model=HDBSCAN(core_dist_n_jobs=1)  # Disable multiprocessing in HDBSCAN\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        topics, probabilities = topic_model.fit_transform(analysis_texts)\n",
    "        \n",
    "        # Get topic information\n",
    "        theme_info = topic_model.get_topic_info()\n",
    "        \n",
    "        print(f\"\\n✓ BERTopic identified {len(theme_info)-1} topics (excluding outliers)\")\n",
    "        \n",
    "    elif model_choice == 'top2vec':\n",
    "        # Initialize Top2Vec model\n",
    "        topic_model = Top2Vec(\n",
    "            documents=analysis_texts,\n",
    "            speed=\"learn\",  # 'fast-learn', 'learn', 'deep-learn'\n",
    "        )\n",
    "        \n",
    "        # Get topic information\n",
    "        topic_sizes, topic_nums = topic_model.get_topic_sizes()\n",
    "        topics_words = topic_model.get_topics()\n",
    "        \n",
    "        # Create theme_info DataFrame\n",
    "        theme_info = pd.DataFrame({\n",
    "            'Topic': topic_nums,\n",
    "            'Count': topic_sizes,\n",
    "            'Name': [', '.join(words[0][:10]) for words in topics_words],  # Top 10 words\n",
    "            'Representation': [words[0][:10] for words in topics_words]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n✓ Top2Vec identified {len(theme_info)} topics\")\n",
    "    \n",
    "    # Display extracted themes\n",
    "    display(theme_info.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"No texts available for analysis. Please check Step 2.\")\n",
    "    theme_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44012421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize topics\n",
    "# Safely get variables that may not exist in the current kernel/session\n",
    "theme_info = globals().get('theme_info', None)\n",
    "model_choice = globals().get('model_choice', 'bertopic')\n",
    "topic_model = globals().get('topic_model', None)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import plotly.io as pio\n",
    "\n",
    "# Prefer a non-blocking renderer for notebooks; HTML embedding fallback is safest\n",
    "try:\n",
    "    pio.renderers.default = 'notebook_connected'\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def _display_fig(fig, title=None):\n",
    "    \"\"\"Embed a Plotly figure as HTML to avoid renderer blocking issues.\"\"\"\n",
    "    try:\n",
    "        html = fig.to_html(full_html=False, include_plotlyjs='cdn')\n",
    "        if title:\n",
    "            display(HTML(f\"<h4>{title}</h4>\"))\n",
    "        display(HTML(html))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to render figure as HTML: {e}\")\n",
    "\n",
    "if theme_info is not None and model_choice == 'bertopic' and topic_model is not None:\n",
    "    print(\"Generating visualizations...\")\n",
    "    \n",
    "    # Topic visualization\n",
    "    try:\n",
    "        fig_topics = topic_model.visualize_topics()\n",
    "        _display_fig(fig_topics, title='Topic Overview')\n",
    "    except Exception as e:\n",
    "        print(f\"Topic visualization failed: {e}\")\n",
    "    \n",
    "    # Topic hierarchy (if enough topics and not too many)\n",
    "    try:\n",
    "        if len(theme_info) > 3 and len(theme_info) < 200:\n",
    "            fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "            _display_fig(fig_hierarchy, title='Topic Hierarchy')\n",
    "        else:\n",
    "            if len(theme_info) >= 200:\n",
    "                print('Skipping hierarchy visualization: too many topics to render.')\n",
    "    except Exception as e:\n",
    "        print(f\"Hierarchy visualization failed: {e}\")\n",
    "    \n",
    "    # Topic distribution\n",
    "    try:\n",
    "        fig_barchart = topic_model.visualize_barchart(top_n_topics=10)\n",
    "        _display_fig(fig_barchart, title='Top Topics Barchart')\n",
    "    except Exception as e:\n",
    "        print(f\"Barchart visualization failed: {e}\")\n",
    "        \n",
    "elif theme_info is not None and model_choice == 'top2vec':\n",
    "    print(\"Top2Vec visualizations not implemented in this notebook.\")\n",
    "    print(\"Consider using BERTopic for richer visualizations.\")\n",
    "    \n",
    "    # Simple bar chart for Top2Vec\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(theme_info)), theme_info['Count'])\n",
    "    plt.xlabel('Topic Number')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.title('Top2Vec Topic Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    if theme_info is None:\n",
    "        print(\"No themes available for visualization. Run the analysis cells first.\")\n",
    "    elif model_choice == 'bertopic' and topic_model is None:\n",
    "        print(\"BERTopic model object not found in the session. Re-run the BERTopic analysis cell.\")\n",
    "    else:\n",
    "        print(\"No visualizations available for the current configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Export themes for validation/import into QDA tool\n",
    "if theme_info is not None:\n",
    "    # Export to CSV\n",
    "    output_file = f\"../data/results/extracted_themes_{model_choice}.csv\"\n",
    "    theme_info.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Themes exported to '{output_file}'\")\n",
    "    \n",
    "    # Export detailed topic information (BERTopic only)\n",
    "    if model_choice == 'bertopic':\n",
    "        # Get topic representations\n",
    "        topic_representations = {}\n",
    "        for topic_id in theme_info['Topic'].unique():\n",
    "            if topic_id != -1:  # Skip outlier topic\n",
    "                words = topic_model.get_topic(topic_id)\n",
    "                topic_representations[topic_id] = words\n",
    "        \n",
    "        # Create detailed export\n",
    "        detailed_themes = []\n",
    "        for topic_id, words in topic_representations.items():\n",
    "            detailed_themes.append({\n",
    "                'topic_id': topic_id,\n",
    "                'topic_name': f\"Topic_{topic_id}\",\n",
    "                'top_words': ', '.join([word for word, _ in words[:10]]),\n",
    "                'word_scores': ', '.join([f\"{word}:{score:.3f}\" for word, score in words[:10]]),\n",
    "                'document_count': theme_info[theme_info['Topic'] == topic_id]['Count'].iloc[0]\n",
    "            })\n",
    "        \n",
    "        detailed_df = pd.DataFrame(detailed_themes)\n",
    "        detailed_output = f\"../data/results/detailed_themes_{model_choice}.csv\"\n",
    "        detailed_df.to_csv(detailed_output, index=False)\n",
    "        print(f\"✓ Detailed themes exported to '{detailed_output}'\")\n",
    "    \n",
    "    # Export text-topic assignments for QDA validation\n",
    "    if model_choice == 'bertopic':\n",
    "        text_topics_df = pd.DataFrame({\n",
    "            'text': analysis_texts,\n",
    "            'source': analysis_sources,\n",
    "            'topic': topics,\n",
    "            'topic_probability': [None] * len(topics)  # Probabilities disabled for memory efficiency\n",
    "        })\n",
    "    else:\n",
    "        # For Top2Vec, we'd need to get document-topic assignments\n",
    "        text_topics_df = pd.DataFrame({\n",
    "            'text': analysis_texts,\n",
    "            'source': analysis_sources,\n",
    "            'topic': ['Top2Vec_topic'] * len(analysis_texts)  # Placeholder\n",
    "        })\n",
    "    \n",
    "    assignments_file = f\"../data/results/text_topic_assignments_{model_choice}.csv\"\n",
    "    text_topics_df.to_csv(assignments_file, index=False)\n",
    "    print(f\"✓ Text-topic assignments exported to '{assignments_file}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPORT COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Files ready for import into QDA tools like:\")\n",
    "    print(\"- Taguette (import CSV files)\")\n",
    "    print(\"- QualCoder (import CSV files)\")\n",
    "    print(\"- NVivo (import CSV files)\")\n",
    "    print(\"- ATLAS.ti (import CSV files)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No themes to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b682b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Validate AI-generated themes** in your QDA tool\n",
    "2. **Refine topics** by merging, splitting, or renaming as needed\n",
    "3. **Code additional texts** using the validated themes\n",
    "4. **Export coded data** for further quantitative analysis\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "- **Preprocessing**: Clean your text data before analysis (remove noise, standardize formats)\n",
    "- **Model Tuning**: Adjust `min_topic_size` in BERTopic for more/fewer topics\n",
    "- **Language**: Set the correct language if your texts aren't in English\n",
    "- **Scale**: For large datasets (>10k texts), consider running on Databricks Community Edition\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Memory issues**: Reduce `min_topic_size` or use Top2Vec instead\n",
    "- **Poor topics**: Preprocess text better or try different model settings\n",
    "- **No texts found**: Check column names in your Excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
